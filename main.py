import requests
from bs4 import BeautifulSoup

# berlinstartupjobs.com ì›¹ì‚¬ì´íŠ¸ìš© ìŠ¤í¬ë˜í¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.
# ìŠ¤í¬ë˜í¼ëŠ” ë‹¤ìŒ URLì„ ìŠ¤í¬ë©í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
# https://berlinstartupjobs.com/engineering/
site = 'https://berlinstartupjobs.com/engineering/'
# https://berlinstartupjobs.com/skill-areas/python/
# https://berlinstartupjobs.com/skill-areas/typescript/
# https://berlinstartupjobs.com/skill-areas/javascript/
# ì²« ë²ˆì§¸ URLì—ëŠ” í˜ì´ì§€ê°€ ìˆìœ¼ë¯€ë¡œ pagination ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
# ë‚˜ë¨¸ì§€ URLì€ íŠ¹ì • ìŠ¤í‚¬ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤. URLì˜ êµ¬ì¡°ì— ìŠ¤í‚¬ ì´ë¦„ì´ ìˆìœ¼ë¯€ë¡œ ëª¨ë“  ìŠ¤í‚¬ì„ ìŠ¤í¬ë˜í•‘í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë˜í¼ë¥¼ ë§Œë“œì„¸ìš”.
# íšŒì‚¬ ì´ë¦„, ì§ë¬´ ì œëª©, ì„¤ëª… ë° ì§ë¬´ ë§í¬ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
r = requests.get(site, headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"})

print(r.status_code)#ì ‘ê·¼ ê°€ëŠ¥200 í™•ì¸

all_jobs = []#ë¦¬ìŠ¤íŠ¸ ìƒì„±

#í˜ì´ì§• í˜ì´ì§€ê°€ ìˆìœ¼ë¯€ë¡œ ìŠ¤í¬ë© ê¸°ëŠ¥ì„ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©
def scrape_page(url, arr):

  #response = requests.get(url)#í˜ì´ì§€ ìš”ì²­
  response = requests.get(url, headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"})

  #print(response.content)#í•´ë‹¹ í˜ì´ì§€ html ì°ìŒ

  soup = BeautifulSoup(response.content, 'html.parser')#html íŒŒì‹±
  jobs = soup.find("ul", class_="jobs-list-items").find_all("li")[0:]

  for job in jobs:
    #íšŒì‚¬ì´ë¦„
    company = job.find("a", class_="bjs-jlid__b").text
    #ì§ë¬´ì œëª©
    title = job.find("h4", class_="bjs-jlid__h").find("a").text
    #ì„¤ëª… bjs-jlid__description div
    desc = job.find("div", class_="bjs-jlid__description").text 
    #ì§ë¬´ë§í¬ bjs-jlid__h ì•ˆì— aíƒœê·¸ href ì†ì„±
    url = job.find("h4", class_="bjs-jlid__h").find("a")["href"]#ë§í¬

    job_data = {
      'title': title,
      'company': company,
      'desc': desc,
      'url': url
    }
    #all_jobs.append(job_data)#ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    arr.append(job_data)#ë¦¬ìŠ¤íŠ¸ì— ì €ì¥

#í˜ì´ì§• ê°œìˆ˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_pages(url):
  #response = requests.get(url)#í˜ì´ì§€ ìš”ì²­
  response = requests.get(url, headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"})
  soup = BeautifulSoup(response.content, "html.parser",)#html íŒŒì‹±
  return len(soup.find("ul", class_= "bsj-nav").find_all("a"))#í˜ì´ì§€ ê°œìˆ˜ë¥¼ ë¦¬í„´

total_pages = get_pages(site)#í˜ì´ì§€ ê°œìˆ˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
print(f"ğŸˆ{site}:{total_pages}")
#í˜ì´ì§€ ê°œìˆ˜ë¥¼ ê°€ì ¸ì™€ ìŠ¤í¬ë© í•¨ìˆ˜ì— ê°œìˆ˜ë¥¼ ë„£ì–´ ë°˜ë³µ í˜¸ì¶œ
for x in range(total_pages):
  url = f"{site}page/{x+1}"#í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” url
  print("ğŸ’§", url)
  scrape_page(url,all_jobs)#í˜ì´ì§€ë¥¼ ìŠ¤í¬ë©í•˜ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œ


print("total",len(all_jobs))#ëª¨ë“  ì •ë³´ë¥¼ ê°€ì ¸ì™”ìœ¼ë¯€ë¡œ ê°œìˆ˜ë¥¼ ì¶œë ¥
print(all_jobs)
print("ğŸ˜ƒí˜ì´ì§€ ìŠ¤í¬ë© ëğŸ˜ƒ")